{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Bank Institution Term Deposit\n",
    "Predictive Model</h1>\n",
    "<p>This notebook attempts the use of XGboost, MLP and Logisitc Regression algorithm to predict customers who would or would not subscribe to their term deposit in the\n",
    "future </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler,MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import imblearn\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV,KFold,StratifiedKFold,train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin,ClassifierMixin\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('./bank-additional-full.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding y values\n",
    "data.y=data.y.map({'no':0,'yes':1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               0\n",
       "job               0\n",
       "marital           0\n",
       "education         0\n",
       "default           0\n",
       "housing           0\n",
       "loan              0\n",
       "contact           0\n",
       "month             0\n",
       "day_of_week       0\n",
       "duration          0\n",
       "campaign          0\n",
       "pdays             0\n",
       "previous          0\n",
       "poutcome          0\n",
       "emp.var.rate      0\n",
       "cons.price.idx    0\n",
       "cons.conf.idx     0\n",
       "euribor3m         0\n",
       "nr.employed       0\n",
       "y                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for missing data\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating new features\n",
    "data['nr.employed_marital']=data.groupby(['marital'])['nr.employed'].transform('sum')\n",
    "data['customer index']=data['cons.price.idx'] * data['cons.conf.idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating instance of one-hot-encoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifying the categorical and non categorical columns\n",
    "cat=[]\n",
    "non_cat=[]\n",
    "for i in data:\n",
    "    if data[i].dtype==object:\n",
    "        cat.append(i)\n",
    "    else:\n",
    "        non_cat.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>...</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>226</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>307</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41183</th>\n",
       "      <td>73</td>\n",
       "      <td>334</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41184</th>\n",
       "      <td>46</td>\n",
       "      <td>383</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41185</th>\n",
       "      <td>56</td>\n",
       "      <td>189</td>\n",
       "      <td>2</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41186</th>\n",
       "      <td>44</td>\n",
       "      <td>442</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41187</th>\n",
       "      <td>74</td>\n",
       "      <td>239</td>\n",
       "      <td>3</td>\n",
       "      <td>999</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>94.767</td>\n",
       "      <td>-50.8</td>\n",
       "      <td>1.028</td>\n",
       "      <td>4963.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41188 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  duration  campaign  pdays  previous  emp.var.rate  cons.price.idx  \\\n",
       "0       56       261         1    999         0           1.1          93.994   \n",
       "1       57       149         1    999         0           1.1          93.994   \n",
       "2       37       226         1    999         0           1.1          93.994   \n",
       "3       40       151         1    999         0           1.1          93.994   \n",
       "4       56       307         1    999         0           1.1          93.994   \n",
       "...    ...       ...       ...    ...       ...           ...             ...   \n",
       "41183   73       334         1    999         0          -1.1          94.767   \n",
       "41184   46       383         1    999         0          -1.1          94.767   \n",
       "41185   56       189         2    999         0          -1.1          94.767   \n",
       "41186   44       442         1    999         0          -1.1          94.767   \n",
       "41187   74       239         3    999         1          -1.1          94.767   \n",
       "\n",
       "       cons.conf.idx  euribor3m  nr.employed  ...   43   44   45   46   47  \\\n",
       "0              -36.4      4.857       5191.0  ...  0.0  0.0  0.0  1.0  0.0   \n",
       "1              -36.4      4.857       5191.0  ...  0.0  0.0  0.0  1.0  0.0   \n",
       "2              -36.4      4.857       5191.0  ...  0.0  0.0  0.0  1.0  0.0   \n",
       "3              -36.4      4.857       5191.0  ...  0.0  0.0  0.0  1.0  0.0   \n",
       "4              -36.4      4.857       5191.0  ...  0.0  0.0  0.0  1.0  0.0   \n",
       "...              ...        ...          ...  ...  ...  ...  ...  ...  ...   \n",
       "41183          -50.8      1.028       4963.6  ...  0.0  0.0  1.0  0.0  0.0   \n",
       "41184          -50.8      1.028       4963.6  ...  0.0  0.0  1.0  0.0  0.0   \n",
       "41185          -50.8      1.028       4963.6  ...  0.0  0.0  1.0  0.0  0.0   \n",
       "41186          -50.8      1.028       4963.6  ...  0.0  0.0  1.0  0.0  0.0   \n",
       "41187          -50.8      1.028       4963.6  ...  0.0  0.0  1.0  0.0  0.0   \n",
       "\n",
       "        48   49   50   51   52  \n",
       "0      0.0  0.0  0.0  1.0  0.0  \n",
       "1      0.0  0.0  0.0  1.0  0.0  \n",
       "2      0.0  0.0  0.0  1.0  0.0  \n",
       "3      0.0  0.0  0.0  1.0  0.0  \n",
       "4      0.0  0.0  0.0  1.0  0.0  \n",
       "...    ...  ...  ...  ...  ...  \n",
       "41183  0.0  0.0  0.0  1.0  0.0  \n",
       "41184  0.0  0.0  0.0  1.0  0.0  \n",
       "41185  0.0  0.0  0.0  1.0  0.0  \n",
       "41186  0.0  0.0  0.0  1.0  0.0  \n",
       "41187  0.0  0.0  1.0  0.0  0.0  \n",
       "\n",
       "[41188 rows x 66 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_df = pd.DataFrame(enc.fit_transform(data[cat]).toarray())\n",
    "# merge with main data on key values\n",
    "newdata=data[non_cat]\n",
    "new_data= newdata.join(enc_df)\n",
    "new_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Handling Outliers</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier(df):\n",
    "    low = .05\n",
    "    high = .95\n",
    "    quant_df = df.quantile([low, high])\n",
    "    for name in list(df.columns):\n",
    "        if is_numeric_dtype(df[name]):\n",
    "            df = df[(df[name] >= quant_df.loc[low, name]) \n",
    "                & (df[name] <= quant_df.loc[high, name])]\n",
    "    return df\n",
    "new_data_0=remove_outlier(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling using MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_scaled=scaler.fit_transform(new_data_0.drop(['y'],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>...</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.96875</td>\n",
       "      <td>0.157821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.700382</td>\n",
       "      <td>0.972727</td>\n",
       "      <td>0.971813</td>\n",
       "      <td>0.75576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.34375</td>\n",
       "      <td>0.265363</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.700382</td>\n",
       "      <td>0.972727</td>\n",
       "      <td>0.971813</td>\n",
       "      <td>0.75576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.43750</td>\n",
       "      <td>0.160615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.700382</td>\n",
       "      <td>0.972727</td>\n",
       "      <td>0.971813</td>\n",
       "      <td>0.75576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.93750</td>\n",
       "      <td>0.378492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.700382</td>\n",
       "      <td>0.972727</td>\n",
       "      <td>0.971813</td>\n",
       "      <td>0.75576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.59375</td>\n",
       "      <td>0.226257</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.700382</td>\n",
       "      <td>0.972727</td>\n",
       "      <td>0.971813</td>\n",
       "      <td>0.75576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  duration  campaign  pdays  previous  emp.var.rate  cons.price.idx  \\\n",
       "0  0.96875  0.157821       0.0    0.0       0.0      0.930233        0.700382   \n",
       "1  0.34375  0.265363       0.0    0.0       0.0      0.930233        0.700382   \n",
       "2  0.43750  0.160615       0.0    0.0       0.0      0.930233        0.700382   \n",
       "3  0.93750  0.378492       0.0    0.0       0.0      0.930233        0.700382   \n",
       "4  0.59375  0.226257       0.0    0.0       0.0      0.930233        0.700382   \n",
       "\n",
       "   cons.conf.idx  euribor3m  nr.employed  ...   43   44   45   46   47   48  \\\n",
       "0       0.972727   0.971813      0.75576  ...  0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "1       0.972727   0.971813      0.75576  ...  0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "2       0.972727   0.971813      0.75576  ...  0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "3       0.972727   0.971813      0.75576  ...  0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "4       0.972727   0.971813      0.75576  ...  0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "\n",
       "    49   50   51   52  \n",
       "0  0.0  0.0  1.0  0.0  \n",
       "1  0.0  0.0  1.0  0.0  \n",
       "2  0.0  0.0  1.0  0.0  \n",
       "3  0.0  0.0  1.0  0.0  \n",
       "4  0.0  0.0  1.0  0.0  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data_scaled=pd.DataFrame(new_data_scaled,columns=new_data_0.drop(['y'],axis=1).columns)\n",
    "new_data_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, we want the explained variance to be between 95–99%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Util.py\n",
    "pca = PCA().fit(new_data_scaled)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xi = np.arange(1, len(pca.explained_variance_ratio_)+1, step=1)\n",
    "y = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.ylim(0.0,1.1)\n",
    "plt.plot(xi, y, marker='o', linestyle='--', color='b')\n",
    "\n",
    "plt.xlabel('Number of Components')\n",
    "plt.xticks(np.arange(0, len(pca.explained_variance_ratio_), step=1),rotation=90) #change from 0-based array index to 1-based human-readable label\n",
    "plt.ylabel('Cumulative variance (%)')\n",
    "plt.title('The number of components needed to explain variance')\n",
    "\n",
    "plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "plt.axhline(y=0.99, color='g', linestyle='-')\n",
    "plt.text(0.5, 1, '99% cut-off threshold', color = 'g', fontsize=16)\n",
    "plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n",
    "\n",
    "ax.grid(axis='x')\n",
    "plt.savefig('PCA.jpg',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows that if we choose the number of component to be 23, we can retain 95% of the useful data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 23)\n",
    "pca.fit(new_data_scaled)\n",
    "reduced = pca.transform(new_data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13820214, 0.23316893, 0.31207808, 0.38524077, 0.44411103,\n",
       "       0.49119352, 0.53757345, 0.58054005, 0.6207757 , 0.65686338,\n",
       "       0.68889751, 0.72018289, 0.75066337, 0.78023173, 0.80835008,\n",
       "       0.83532741, 0.85891909, 0.88207007, 0.90178741, 0.91855478,\n",
       "       0.9314657 , 0.9430439 , 0.95422244])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#xi = np.arange(1, len(pca.explained_variance_ratio_)+1, step=1)\n",
    "y = np.cumsum(pca.explained_variance_ratio_)\n",
    "y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an imbalanced dataset, we are going to employ the imblearn libary to resample the dataset using the following \n",
    "<p>SMOTE</p>\n",
    "<p>TOMEKLINKS</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=reduced\n",
    "y=new_data_0['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22341"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Under sampling\n",
    "tl = imblearn.under_sampling.TomekLinks(sampling_strategy='majority')\n",
    "X_tl, y_tl = tl.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Over sampling\n",
    "smote = SMOTE(sampling_strategy='minority')\n",
    "X_sm, y_sm = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Under sampling didn't provide a good fit\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_sm,y_sm,test_size=0.1,random_state=1234,shuffle=True)\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.1,random_state=1234,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xf_train,Xf_test,yf_train,yf_test=train_test_split(X_sm,y_sm,test_size=0.1,random_state=1234,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Data.py\n",
    "class Preprocessing(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, data, y=None):\n",
    "        return self\n",
    "    def remove_outlier(self, df):\n",
    "        low = .05\n",
    "        high = .95\n",
    "        quant_df = df.quantile([low, high])\n",
    "        for name in list(df.columns):\n",
    "            if is_numeric_dtype(df[name]):\n",
    "                df = df[(df[name] >= quant_df.loc[low, name]) \n",
    "                    & (df[name] <= quant_df.loc[high, name])]\n",
    "        return df\n",
    "\n",
    "    def transform(self,data):\n",
    "        scaler=MinMaxScaler()\n",
    "        enc = OneHotEncoder(handle_unknown='ignore')\n",
    "        cat=[]\n",
    "        non_cat=[]\n",
    "        \n",
    "        if 'y' in data.columns:\n",
    "            data.y=data.y.map({'no':0,'yes':1})\n",
    "            for i in data:\n",
    "                if data[i].dtype==object:\n",
    "                    cat.append(i)\n",
    "                else:\n",
    "                    non_cat.append(i)\n",
    "            enc_df = pd.DataFrame(enc.fit_transform(data[cat]).toarray())\n",
    "            # merge with main data on key values\n",
    "            newdata=data[non_cat]\n",
    "            newdata= newdata.join(enc_df)\n",
    "            newdata=self.remove_outlier(newdata)\n",
    "            new_data_scaled=scaler.fit_transform(newdata.drop(['y'],axis=1))\n",
    "            new_data_scaled=pd.DataFrame(new_data_scaled,columns=newdata.drop(['y'],axis=1).columns)\n",
    "        else:\n",
    "            for i in data:\n",
    "                if data[i].dtype==object:\n",
    "                    cat.append(i)\n",
    "                else:\n",
    "                    non_cat.append(i)\n",
    "            enc_df = pd.DataFrame(enc.fit_transform(data[cat]).toarray())\n",
    "            # merge with main data on key values\n",
    "            newdata=data[non_cat]\n",
    "            newdata= newdata.join(enc_df)\n",
    "            newdata=self.remove_outlier(newdata)\n",
    "            new_data_scaled=scaler.fit_transform(newdata)\n",
    "            new_data_scaled=pd.DataFrame(new_data_scaled,columns=newdata.columns)\n",
    "        pca = PCA(n_components = 23)\n",
    "        pca.fit(new_data_scaled)\n",
    "        reduced = pca.transform(new_data_scaled)\n",
    "        reduced=pd.DataFrame(reduced,columns=range(reduced.shape[1]))\n",
    "        reduced['y']=newdata['y'].values\n",
    "        return reduced\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Both kfold and Stratifiedfold strategy gave similar performance\n",
    "kfold = KFold(n_splits=5)\n",
    "stratfold=StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGB ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define eval metrics\n",
    "\n",
    "def xgb_f1(y, t, threshold=0.5):\n",
    "    try:\n",
    "        t = t.get_label()\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    y_bin = (y > threshold).astype(int) # works for both type(y) == <class 'numpy.ndarray'> and type(y) == <class 'pandas.core.series.Series'>\n",
    "    return 'f1',f1_score(t,y_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:26:48] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { n_estimators, nfold, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-error:0.21784\teval-error:0.23982\ttrain-f1:0.80338\teval-f1:0.78332\n",
      "Multiple eval metrics have been passed: 'eval-f1' will be used for early stopping.\n",
      "\n",
      "Will train until eval-f1 hasn't improved in 100 rounds.\n",
      "[1]\ttrain-error:0.19903\teval-error:0.22349\ttrain-f1:0.81614\teval-f1:0.79329\n",
      "[2]\ttrain-error:0.15531\teval-error:0.17604\ttrain-f1:0.84784\teval-f1:0.82558\n",
      "[3]\ttrain-error:0.13114\teval-error:0.15763\ttrain-f1:0.87204\teval-f1:0.84482\n",
      "[4]\ttrain-error:0.11313\teval-error:0.14234\ttrain-f1:0.88959\teval-f1:0.86062\n",
      "[5]\ttrain-error:0.10284\teval-error:0.13741\ttrain-f1:0.90017\teval-f1:0.86657\n",
      "[6]\ttrain-error:0.09399\teval-error:0.12601\ttrain-f1:0.90915\teval-f1:0.87758\n",
      "[7]\ttrain-error:0.09083\teval-error:0.12367\ttrain-f1:0.91203\teval-f1:0.87988\n",
      "[8]\ttrain-error:0.09002\teval-error:0.12315\ttrain-f1:0.91307\teval-f1:0.88098\n",
      "[9]\ttrain-error:0.09120\teval-error:0.12108\ttrain-f1:0.91174\teval-f1:0.88293\n",
      "[10]\ttrain-error:0.09140\teval-error:0.11952\ttrain-f1:0.91156\teval-f1:0.88408\n",
      "[11]\ttrain-error:0.09385\teval-error:0.12186\ttrain-f1:0.90891\teval-f1:0.88143\n",
      "[12]\ttrain-error:0.08924\teval-error:0.11434\ttrain-f1:0.91345\teval-f1:0.88861\n",
      "[13]\ttrain-error:0.08693\teval-error:0.11304\ttrain-f1:0.91582\teval-f1:0.89034\n",
      "[14]\ttrain-error:0.08740\teval-error:0.11200\ttrain-f1:0.91526\teval-f1:0.89129\n",
      "[15]\ttrain-error:0.08806\teval-error:0.11174\ttrain-f1:0.91440\teval-f1:0.89080\n",
      "[16]\ttrain-error:0.08875\teval-error:0.11382\ttrain-f1:0.91355\teval-f1:0.88883\n",
      "[17]\ttrain-error:0.08904\teval-error:0.11382\ttrain-f1:0.91328\teval-f1:0.88872\n",
      "[18]\ttrain-error:0.08918\teval-error:0.11589\ttrain-f1:0.91320\teval-f1:0.88675\n",
      "[19]\ttrain-error:0.08939\teval-error:0.11641\ttrain-f1:0.91284\teval-f1:0.88624\n",
      "[20]\ttrain-error:0.08987\teval-error:0.11693\ttrain-f1:0.91226\teval-f1:0.88562\n",
      "[21]\ttrain-error:0.09155\teval-error:0.11667\ttrain-f1:0.91058\teval-f1:0.88584\n",
      "[22]\ttrain-error:0.08924\teval-error:0.11563\ttrain-f1:0.91294\teval-f1:0.88697\n",
      "[23]\ttrain-error:0.09085\teval-error:0.11797\ttrain-f1:0.91124\teval-f1:0.88437\n",
      "[24]\ttrain-error:0.08815\teval-error:0.11615\ttrain-f1:0.91406\teval-f1:0.88618\n",
      "[25]\ttrain-error:0.08650\teval-error:0.11304\ttrain-f1:0.91577\teval-f1:0.88951\n",
      "[26]\ttrain-error:0.08549\teval-error:0.11174\ttrain-f1:0.91674\teval-f1:0.89086\n",
      "[27]\ttrain-error:0.08541\teval-error:0.11278\ttrain-f1:0.91683\teval-f1:0.88979\n",
      "[28]\ttrain-error:0.08544\teval-error:0.11304\ttrain-f1:0.91670\teval-f1:0.88945\n",
      "[29]\ttrain-error:0.08619\teval-error:0.11174\ttrain-f1:0.91592\teval-f1:0.89053\n",
      "[30]\ttrain-error:0.08737\teval-error:0.11434\ttrain-f1:0.91475\teval-f1:0.88793\n",
      "[31]\ttrain-error:0.08708\teval-error:0.11589\ttrain-f1:0.91504\teval-f1:0.88640\n",
      "[32]\ttrain-error:0.08757\teval-error:0.11563\ttrain-f1:0.91455\teval-f1:0.88680\n",
      "[33]\ttrain-error:0.08740\teval-error:0.11641\ttrain-f1:0.91466\teval-f1:0.88595\n",
      "[34]\ttrain-error:0.08884\teval-error:0.11615\ttrain-f1:0.91323\teval-f1:0.88624\n",
      "[35]\ttrain-error:0.08895\teval-error:0.11511\ttrain-f1:0.91307\teval-f1:0.88714\n",
      "[36]\ttrain-error:0.08806\teval-error:0.11434\ttrain-f1:0.91394\teval-f1:0.88793\n",
      "[37]\ttrain-error:0.08878\teval-error:0.11382\ttrain-f1:0.91315\teval-f1:0.88844\n",
      "[38]\ttrain-error:0.08682\teval-error:0.11226\ttrain-f1:0.91513\teval-f1:0.89013\n",
      "[39]\ttrain-error:0.08590\teval-error:0.11174\ttrain-f1:0.91599\teval-f1:0.89075\n",
      "[40]\ttrain-error:0.08630\teval-error:0.11252\ttrain-f1:0.91558\teval-f1:0.89002\n",
      "[41]\ttrain-error:0.08743\teval-error:0.11149\ttrain-f1:0.91447\teval-f1:0.89086\n",
      "[42]\ttrain-error:0.08521\teval-error:0.10941\ttrain-f1:0.91672\teval-f1:0.89289\n",
      "[43]\ttrain-error:0.08475\teval-error:0.10915\ttrain-f1:0.91720\teval-f1:0.89312\n",
      "[44]\ttrain-error:0.08425\teval-error:0.10941\ttrain-f1:0.91767\teval-f1:0.89289\n",
      "[45]\ttrain-error:0.08434\teval-error:0.11045\ttrain-f1:0.91755\teval-f1:0.89188\n",
      "[46]\ttrain-error:0.08336\teval-error:0.10889\ttrain-f1:0.91855\teval-f1:0.89345\n",
      "[47]\ttrain-error:0.08137\teval-error:0.10682\ttrain-f1:0.92054\teval-f1:0.89564\n",
      "[48]\ttrain-error:0.08206\teval-error:0.10786\ttrain-f1:0.91985\teval-f1:0.89447\n",
      "[49]\ttrain-error:0.08106\teval-error:0.10760\ttrain-f1:0.92090\teval-f1:0.89486\n",
      "[50]\ttrain-error:0.08037\teval-error:0.10812\ttrain-f1:0.92158\teval-f1:0.89430\n",
      "[51]\ttrain-error:0.08016\teval-error:0.10863\ttrain-f1:0.92170\teval-f1:0.89379\n",
      "[52]\ttrain-error:0.08037\teval-error:0.10863\ttrain-f1:0.92143\teval-f1:0.89368\n",
      "[53]\ttrain-error:0.08057\teval-error:0.10837\ttrain-f1:0.92129\teval-f1:0.89385\n",
      "[54]\ttrain-error:0.08117\teval-error:0.10812\ttrain-f1:0.92062\teval-f1:0.89419\n",
      "[55]\ttrain-error:0.08002\teval-error:0.10837\ttrain-f1:0.92181\teval-f1:0.89391\n",
      "[56]\ttrain-error:0.08002\teval-error:0.10786\ttrain-f1:0.92187\teval-f1:0.89458\n",
      "[57]\ttrain-error:0.07921\teval-error:0.10656\ttrain-f1:0.92261\teval-f1:0.89587\n",
      "[58]\ttrain-error:0.07898\teval-error:0.10708\ttrain-f1:0.92284\teval-f1:0.89520\n",
      "[59]\ttrain-error:0.07898\teval-error:0.10708\ttrain-f1:0.92279\teval-f1:0.89515\n",
      "[60]\ttrain-error:0.07866\teval-error:0.10708\ttrain-f1:0.92309\teval-f1:0.89520\n",
      "[61]\ttrain-error:0.07878\teval-error:0.10682\ttrain-f1:0.92299\teval-f1:0.89538\n",
      "[62]\ttrain-error:0.07792\teval-error:0.10578\ttrain-f1:0.92390\teval-f1:0.89639\n",
      "[63]\ttrain-error:0.07740\teval-error:0.10578\ttrain-f1:0.92442\teval-f1:0.89629\n",
      "[64]\ttrain-error:0.07734\teval-error:0.10578\ttrain-f1:0.92448\teval-f1:0.89629\n",
      "[65]\ttrain-error:0.07676\teval-error:0.10397\ttrain-f1:0.92507\teval-f1:0.89815\n",
      "[66]\ttrain-error:0.07745\teval-error:0.10423\ttrain-f1:0.92439\teval-f1:0.89781\n",
      "[67]\ttrain-error:0.07604\teval-error:0.10552\ttrain-f1:0.92582\teval-f1:0.89652\n",
      "[68]\ttrain-error:0.07619\teval-error:0.10552\ttrain-f1:0.92564\teval-f1:0.89657\n",
      "[69]\ttrain-error:0.07472\teval-error:0.10345\ttrain-f1:0.92713\teval-f1:0.89855\n",
      "[70]\ttrain-error:0.07325\teval-error:0.10137\ttrain-f1:0.92861\teval-f1:0.90063\n",
      "[71]\ttrain-error:0.07371\teval-error:0.10163\ttrain-f1:0.92814\teval-f1:0.90036\n",
      "[72]\ttrain-error:0.07250\teval-error:0.10111\ttrain-f1:0.92936\teval-f1:0.90086\n",
      "[73]\ttrain-error:0.07207\teval-error:0.10163\ttrain-f1:0.92981\teval-f1:0.90046\n",
      "[74]\ttrain-error:0.07158\teval-error:0.10137\ttrain-f1:0.93029\teval-f1:0.90079\n",
      "[75]\ttrain-error:0.07089\teval-error:0.10137\ttrain-f1:0.93101\teval-f1:0.90084\n",
      "[76]\ttrain-error:0.07017\teval-error:0.10137\ttrain-f1:0.93170\teval-f1:0.90084\n",
      "[77]\ttrain-error:0.06855\teval-error:0.09982\ttrain-f1:0.93329\teval-f1:0.90241\n",
      "[78]\ttrain-error:0.06815\teval-error:0.09800\ttrain-f1:0.93371\teval-f1:0.90421\n",
      "[79]\ttrain-error:0.06743\teval-error:0.09567\ttrain-f1:0.93442\teval-f1:0.90656\n",
      "[80]\ttrain-error:0.06691\teval-error:0.09541\ttrain-f1:0.93495\teval-f1:0.90693\n",
      "[81]\ttrain-error:0.06619\teval-error:0.09463\ttrain-f1:0.93565\teval-f1:0.90753\n",
      "[82]\ttrain-error:0.06573\teval-error:0.09360\ttrain-f1:0.93611\teval-f1:0.90854\n",
      "[83]\ttrain-error:0.06504\teval-error:0.09230\ttrain-f1:0.93680\teval-f1:0.90992\n",
      "[84]\ttrain-error:0.06446\teval-error:0.09152\ttrain-f1:0.93738\teval-f1:0.91056\n",
      "[85]\ttrain-error:0.06426\teval-error:0.09178\ttrain-f1:0.93758\teval-f1:0.91033\n",
      "[86]\ttrain-error:0.06400\teval-error:0.09100\ttrain-f1:0.93785\teval-f1:0.91112\n",
      "[87]\ttrain-error:0.06391\teval-error:0.09074\ttrain-f1:0.93791\teval-f1:0.91139\n",
      "[88]\ttrain-error:0.06409\teval-error:0.09048\ttrain-f1:0.93775\teval-f1:0.91171\n",
      "[89]\ttrain-error:0.06313\teval-error:0.08971\ttrain-f1:0.93870\teval-f1:0.91241\n",
      "[90]\ttrain-error:0.06322\teval-error:0.09048\ttrain-f1:0.93862\teval-f1:0.91162\n",
      "[91]\ttrain-error:0.06233\teval-error:0.09074\ttrain-f1:0.93954\teval-f1:0.91157\n",
      "[92]\ttrain-error:0.06230\teval-error:0.09074\ttrain-f1:0.93958\teval-f1:0.91157\n",
      "[93]\ttrain-error:0.06204\teval-error:0.09048\ttrain-f1:0.93981\teval-f1:0.91185\n",
      "[94]\ttrain-error:0.06210\teval-error:0.08893\ttrain-f1:0.93977\teval-f1:0.91341\n",
      "[95]\ttrain-error:0.06123\teval-error:0.08867\ttrain-f1:0.94062\teval-f1:0.91372\n",
      "[96]\ttrain-error:0.06080\teval-error:0.08763\ttrain-f1:0.94106\teval-f1:0.91478\n",
      "[97]\ttrain-error:0.06051\teval-error:0.08789\ttrain-f1:0.94135\teval-f1:0.91450\n",
      "[98]\ttrain-error:0.05979\teval-error:0.08711\ttrain-f1:0.94206\teval-f1:0.91536\n",
      "[99]\ttrain-error:0.05945\teval-error:0.08686\ttrain-f1:0.94239\teval-f1:0.91560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttrain-error:0.05916\teval-error:0.08608\ttrain-f1:0.94267\teval-f1:0.91625\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-error:0.21784\teval-error:0.23982\ttrain-f1:0.80338\teval-f1:0.78332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\"objective\": \"binary:logistic\", # for classification\n",
    "          \"booster\" : \"gbtree\",   # use tree based models \n",
    "          \"eta\": 0.01,   # learning rate\n",
    "          \"max_depth\": 10,    # maximum depth of a tree\n",
    "          \"subsample\": 1.0,    # Subsample ratio of the training instances\n",
    "          \"colsample_bytree\": 0.7,   # Subsample ratio of columns when constructing each tree\n",
    "          \"silent\": 1,   # silent mode\n",
    "          \"seed\": 10 ,  # Random number seed\n",
    "           \"n_estimators\":300,\n",
    "          \"nfold\":kfold\n",
    "          }\n",
    "num_boost_round = 4000\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dvalid = xgb.DMatrix(X_val, y_val)\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "# train the xgboost model\n",
    "model = xgb.train(params, dtrain, num_boost_round, evals=watchlist, \\\n",
    "  early_stopping_rounds= 100,feval=xgb_f1,verbose_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('f1', 0.9162462159434914)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(xgb.DMatrix(X_val))\n",
    "xgb_f1(y_pred,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:37:32] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { n_estimators, nfold, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#yf_train=pf.DataFRame(yf_train,coumns=new_data_scaled.columns[-1])\n",
    "dtrain = xgb.DMatrix(Xf_train,yf_train)\n",
    "dtest = xgb.DMatrix(Xf_test)\n",
    "params = {\"objective\": \"binary:logistic\", # for classification\n",
    "          \"booster\" : \"gbtree\",   # use tree based models \n",
    "          \"eta\": 0.01,   # learning rate\n",
    "          \"max_depth\": 10,    # maximum depth of a tree\n",
    "          \"subsample\": 1.0,    # Subsample ratio of the training instances\n",
    "          \"colsample_bytree\": 0.7,   # Subsample ratio of columns when constructing each tree\n",
    "          \"silent\": 1,   # silent mode\n",
    "          \"seed\": 10 ,  # Random number seed\n",
    "           \"n_estimators\":300,\n",
    "          \"nfold\":kfold\n",
    "          }\n",
    "num_round = 1000\n",
    "\n",
    "\n",
    "\n",
    "# train the xgboost model\n",
    "model = xgb.train(params, dtrain, num_round)\n",
    "# make prediction\n",
    "preds = model.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        x_train_total, y_train_total = X,y\n",
    "        dtrain = xgb.DMatrix(x_train_total, y_train_total)\n",
    "        # specify parameters via map\n",
    "        params = {\"objective\": \"binary:logistic\", # for classification\n",
    "          \"booster\" : \"gbtree\",   # use tree based models \n",
    "          \"eta\": 0.01,   # learning rate\n",
    "          \"max_depth\": 10,    # maximum depth of a tree\n",
    "          \"subsample\": 1.0,    # Subsample ratio of the training instances\n",
    "          \"colsample_bytree\": 0.7,   # Subsample ratio of columns when constructing each tree\n",
    "          \"silent\": 1,   # silent mode\n",
    "          \"seed\": 10 ,  # Random number seed\n",
    "           \"n_estimators\":300,\n",
    "          }\n",
    "        num_round = 2000\n",
    "        self.model = xgb.train(params, dtrain, num_round)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        dtest = xgb.DMatrix(X)\n",
    "        predict=np.expm1(0.995*(self.model.predict(dtest)))\n",
    "        return predict\n",
    "    def metric(self,y, t, threshold=0.5):\n",
    "        try:\n",
    "            t = t.get_label()\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        y_bin = (y > threshold).astype(int) # works for both type(y) == <class 'numpy.ndarray'> and type(y) == <class 'pandas.core.series.Series'>\n",
    "        return 'f1',f1_score(t,y_bin)\n",
    "    def score(self,X,y):\n",
    "        y_pred=self.predict(X)\n",
    "        score=self.metric(y_pred,y)\n",
    "        return score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:48:26] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { n_estimators, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=XGBClassifier()\n",
    "model.fit(Xf_train,yf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('f1', 0.9653890824622532)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('f1', 0.9653890824622532)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(Xf_test)\n",
    "print(xgb_f1(y_pred,yf_test))\n",
    "model.score(Xf_test,yf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model to pickle\n",
    "pickle.dump(model, open('xgb.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        stratfold=StratifiedKFold(n_splits=5)\n",
    "        # Create logistic regression\n",
    "        logistic = LogisticRegression()\n",
    "        # Create regularization penalty space\n",
    "        penalty = ['l1', 'l2']\n",
    "        # Create regularization hyperparameter space\n",
    "        C = np.logspace(0, 4, 100)\n",
    "\n",
    "        # Create hyperparameter options\n",
    "        hyperparameters = dict(C=C, penalty=penalty)\n",
    "        clf = GridSearchCV(logistic, hyperparameters, cv=stratfold, verbose=0,scoring='f1')\n",
    "        self.model = clf.fit(X_train, y_train)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predict=self.model.predict(X)\n",
    "        return predict\n",
    "    def score(self,X,y):\n",
    "        score=self.model.score(X,y)\n",
    "        return score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LogRegClassifier()\n",
    "model.fit(Xf_train,yf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6490299823633157"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(Xf_test,yf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logistic regression\n",
    "logistic = LogisticRegression()\n",
    "# Create regularization penalty space\n",
    "penalty = ['l1', 'l2']\n",
    "# Create regularization hyperparameter space\n",
    "C = np.logspace(0, 4, 100)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(logistic, hyperparameters, cv=stratfold, verbose=0,scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View best hyperparameters\n",
    "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.670632774472688"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.score(X_val,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLPCLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9224704336399474\n",
      "{'solver': 'lbfgs', 'random_state': 3, 'max_iter': 2000, 'hidden_layer_sizes': 13, 'alpha': 1e-07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "parameters = {'solver': ['lbfgs'], 'max_iter': [1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000 ], 'alpha': 10.0 ** -np.arange(1, 10), 'hidden_layer_sizes':np.arange(10, 15), 'random_state':[0,1,2,3,4,5,6,7,8,9]}\n",
    "clf = RandomizedSearchCV(MLPClassifier(), parameters, cv=kfold,n_jobs=-1,scoring='f1')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_train, y_train))\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        stratfold=StratifiedKFold(n_splits=5)\n",
    "        parameters = {'solver': ['lbfgs'], 'max_iter': [1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000 ], \n",
    "                      'alpha': 10.0 ** -np.arange(1, 10), 'hidden_layer_sizes':np.arange(10, 15), \n",
    "                      'random_state':[0,1,2,3,4,5,6,7,8,9]}\n",
    "        clf = RandomizedSearchCV(MLPClassifier(), parameters, cv=stratfold,n_jobs=-1,scoring='f1')\n",
    "        self.model = clf.fit(X_train, y_train)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predict=self.model.predict(X)\n",
    "        return predict\n",
    "    def score(self,X,y):\n",
    "        score=self.model.score(X,y)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp=MLPClassifier()\n",
    "model_mlp.fit(Xf_train,yf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9507584597432905"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best_params_=[{'solver': 'lbfgs', 'random_state': 3, 'max_iter': 2000, 'hidden_layer_sizes': 13, 'alpha': 1e-07}]\n",
    "model_mlp.score(Xf_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification models Combining models into Model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Model.py\n",
    "class XGBClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        x_train_total, y_train_total = X,y\n",
    "        dtrain = xgb.DMatrix(x_train_total, y_train_total)\n",
    "        # specify parameters via map\n",
    "        params = {\"objective\": \"binary:logistic\", # for classification\n",
    "          \"booster\" : \"gbtree\",   # use tree based models \n",
    "          \"eta\": 0.01,   # learning rate\n",
    "          \"max_depth\": 10,    # maximum depth of a tree\n",
    "          \"subsample\": 1.0,    # Subsample ratio of the training instances\n",
    "          \"colsample_bytree\": 0.7,   # Subsample ratio of columns when constructing each tree\n",
    "          \"silent\": 1,   # silent mode\n",
    "          \"seed\": 10 ,  # Random number seed\n",
    "           \"n_estimators\":300,\n",
    "          }\n",
    "        num_round = 2000\n",
    "        self.model = xgb.train(params, dtrain, num_round)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        dtest = xgb.DMatrix(X)\n",
    "        predict=np.expm1(0.995*(self.model.predict(dtest)))\n",
    "        return predict\n",
    "    def metric(self,y, t, threshold=0.5):\n",
    "        try:\n",
    "            t = t.get_label()\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        y_bin = (y > threshold).astype(int) # works for both type(y) == <class 'numpy.ndarray'> and type(y) == <class 'pandas.core.series.Series'>\n",
    "        return 'f1',f1_score(t,y_bin)\n",
    "    def score(self,X,y):\n",
    "        y_pred=self.predict(X)\n",
    "        score=self.metric(y_pred,y)\n",
    "        return score\n",
    "        \n",
    "class LogRegClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        stratfold=StratifiedKFold(n_splits=5)\n",
    "        # Create logistic regression\n",
    "        logistic = LogisticRegression()\n",
    "        # Create regularization penalty space\n",
    "        penalty = ['l1', 'l2']\n",
    "        # Create regularization hyperparameter space\n",
    "        C = np.logspace(0, 4, 100)\n",
    "\n",
    "        # Create hyperparameter options\n",
    "        hyperparameters = dict(C=C, penalty=penalty)\n",
    "        clf = GridSearchCV(logistic, hyperparameters, cv=stratfold, verbose=0,scoring='f1')\n",
    "        self.model = clf.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predict=self.model.predict(X)\n",
    "        return predict\n",
    "    def score(self,X,y):\n",
    "        score=self.model.score(X,y)\n",
    "        return score\n",
    "        \n",
    "class MultiLayerClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        stratfold=StratifiedKFold(n_splits=5)\n",
    "        parameters = {'solver': ['lbfgs'], 'max_iter': [1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000 ], \n",
    "                      'alpha': 10.0 ** -np.arange(1, 10), 'hidden_layer_sizes':np.arange(10, 15), \n",
    "                      'random_state':[0,1,2,3,4,5,6,7,8,9]}\n",
    "        clf = RandomizedSearchCV(MLPClassifier(), parameters, cv=stratfold,n_jobs=-1,scoring='f1')\n",
    "        self.model = clf.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predict=self.model.predict(X)\n",
    "        return predict\n",
    "    def score(self,X,y):\n",
    "        score=self.model.score(X,y)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Main.py\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler,MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV,KFold,StratifiedKFold,train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin,ClassifierMixin\n",
    "from Model import MLPClassifier,XGBClassifier,LogRegClassifier\n",
    "from Data import Preprocessing\n",
    "import pickle\n",
    "def model():\n",
    "    data=pd.read_csv('./bank-additional-full.csv',sep=';')\n",
    "    reduced_data=Preprocessing().fit_transform(data)\n",
    "    smote = SMOTE(sampling_strategy='minority')\n",
    "    X_sm, y_sm = smote.fit_resample(reduced_data.drop(['y'],axis=1), reduced_data.y)\n",
    "    Xf_train,Xf_test,yf_train,yf_test=train_test_split(X_sm,y_sm,test_size=0.1,random_state=1234,shuffle=True)\n",
    "    model_mlp=MultiLayerClassifier()\n",
    "    model_xgb=XGBClassifier()\n",
    "    model_logreg=LogRegClassifier()\n",
    "    model_mlp.fit(Xf_train,yf_train)\n",
    "    model_xgb.fit(Xf_train,yf_train)\n",
    "    model_logreg.fit(Xf_train,yf_train)\n",
    "    print (\"The accuracy(f1 score) for {0} mode is {1}\".format('Multilayer perceptron',model_mlp.score(Xf_test,yf_test)))\n",
    "    print (\"The accuracy(f1 score) for {0} mode is {1}\".format('XGBClassifier',model_xgb.score(Xf_test,yf_test)))\n",
    "    print (\"The accuracy(f1 score) for {0} mode is {1}\".format('Logistic Regression',model_logreg.score(Xf_test,yf_test)))\n",
    "model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
